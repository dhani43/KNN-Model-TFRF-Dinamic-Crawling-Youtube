{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMCGp5NQuSa9vDfLgErZxJ5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dhani43/KNN-Model-TFRF-Dinamic-Crawling-Youtube/blob/main/Implementasi_Sistem.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMFFTlnzPrJZ"
      },
      "source": [
        "**1. INSTALL REQUIREMENT**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yPFPDXrPzN5v"
      },
      "outputs": [],
      "source": [
        "!pip install pandas numpy scikit-learn openpyxl nltk google-api-python-client Sastrawi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IwL36FB9QaLC"
      },
      "source": [
        "**2. IMPORT REQUIREMENT**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "import pickle\n",
        "import os\n",
        "import time\n",
        "from datetime import datetime\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from google.colab import files, drive\n",
        "from googleapiclient.discovery import build"
      ],
      "metadata": {
        "id": "7WJsKzRVqNKD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0aIxgpQWxrG"
      },
      "source": [
        "**3. FUNGSI LOAD MODEL KNN, PREPROCESSING TEXT, CRAWLING DATA YOUTUBE, DAN PREDIKSI SENTIMEN**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fungsi untuk memuat model dan vectorizer\n",
        "def load_models():\n",
        "    with open(drive_path + \"knn_sentiment.pkl\", \"rb\") as f:\n",
        "        sentiment_model = pickle.load(f)\n",
        "    with open(drive_path + \"knn_function.pkl\", \"rb\") as f:\n",
        "        function_model = pickle.load(f)\n",
        "    with open(drive_path + \"vectorizer.pkl\", \"rb\") as f:\n",
        "        vectorizer = pickle.load(f)\n",
        "    with open(drive_path + \"tf_transformer.pkl\", \"rb\") as f:\n",
        "        tf_transformer = pickle.load(f)\n",
        "    with open(drive_path + \"sentiment_encoder.pkl\", \"rb\") as f:\n",
        "        sentiment_encoder = pickle.load(f)\n",
        "    with open(drive_path + \"function_encoder.pkl\", \"rb\") as f:\n",
        "        function_encoder = pickle.load(f)\n",
        "\n",
        "    print(\"‚úÖ Model dan vectorizer berhasil dimuat!\")\n",
        "    return sentiment_model, function_model, vectorizer, tf_transformer, sentiment_encoder, function_encoder\n",
        "\n",
        "# Fungsi untuk membersihkan teks\n",
        "def preprocesses_text(text):\n",
        "    casefolded_text = text.lower()\n",
        "    cleaned_text = re.sub(r'@\\w+|http\\S+|www\\.\\S+|<.*?>|[^\\w\\s]', ' ', casefolded_text)\n",
        "    cleaned_text = cleaned_text.strip()\n",
        "    tokens = word_tokenize(cleaned_text)\n",
        "    filtered = [word for word in tokens if word not in stop_words]\n",
        "    stemmed = [stemmer.stem(word) for word in filtered]\n",
        "    final_text = ' '.join(stemmed)\n",
        "    return {\n",
        "        'casefolded_text': casefolded_text,\n",
        "        'cleaned_text': cleaned_text,\n",
        "        'tokens': tokens,\n",
        "        'filtered': filtered,\n",
        "        'stemmed': stemmed,\n",
        "        'final_text': final_text\n",
        "    }\n",
        "\n",
        "results = df[text_column].astype(str).apply(preprocesses_text)\n",
        "df['casefolded_text'] = results.apply(lambda x: x['casefolded_text'])\n",
        "df['cleaned_text'] = results.apply(lambda x: x['cleaned_text'])\n",
        "df['tokens'] = results.apply(lambda x: x['tokens'])\n",
        "df['filtered'] = results.apply(lambda x: x['filtered'])\n",
        "df['stemmed'] = results.apply(lambda x: x['stemmed'])\n",
        "df['final_text'] = results.apply(lambda x: x['final_text'])\n",
        "\n",
        "# Fungsi untuk mengambil komentar dari YouTube\n",
        "def get_video_comments(api_key, video_id):\n",
        "    try:\n",
        "        youtube = build('youtube', 'v3', developerKey=api_key)\n",
        "        all_comments = []\n",
        "        next_page_token = None\n",
        "        count = 0\n",
        "\n",
        "        while True:\n",
        "            response = youtube.commentThreads().list(\n",
        "                part=\"snippet\",\n",
        "                videoId=video_id,\n",
        "                textFormat=\"plainText\",\n",
        "                pageToken=next_page_token\n",
        "            ).execute()\n",
        "\n",
        "            for item in response.get(\"items\", []):\n",
        "                comment = item[\"snippet\"][\"topLevelComment\"][\"snippet\"][\"textDisplay\"]\n",
        "                processed = preprocesses_text(comment)\n",
        "                all_comments.append(processed['final_text'])\n",
        "                count += 1\n",
        "\n",
        "            next_page_token = response.get(\"nextPageToken\")\n",
        "            if not next_page_token:\n",
        "                break\n",
        "\n",
        "        print(f\"\\n‚úÖ Berhasil mengambil {count} komentar dari video YouTube.\")\n",
        "\n",
        "        if not all_comments:\n",
        "            print(\"‚ö† Tidak ada komentar yang ditemukan.\")\n",
        "\n",
        "        return all_comments\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Terjadi kesalahan saat mengambil komentar: {e}\")\n",
        "        return []\n",
        "\n",
        "# Fungsi untuk melakukan prediksi sentimen dan fungsi\n",
        "def predict_comments(comments, sentiment_model, function_model, vectorizer, tf_transformer, sentiment_encoder, function_encoder):\n",
        "    if not comments:\n",
        "        print(\"‚ö† Tidak ada komentar yang dapat diprediksi.\")\n",
        "        return []\n",
        "\n",
        "    # Vectorisasi komentar\n",
        "    X_counts = vectorizer.transform(comments)\n",
        "    X_tf = tf_transformer.transform(X_counts)\n",
        "\n",
        "    # Prediksi sentimen\n",
        "    sentiment_predictions = sentiment_model.predict(X_tf)\n",
        "    sentiment_labels = sentiment_encoder.inverse_transform(sentiment_predictions)\n",
        "\n",
        "    # Prediksi fungsi\n",
        "    function_predictions = function_model.predict(X_tf)\n",
        "    function_labels = function_encoder.inverse_transform(function_predictions)\n",
        "\n",
        "    print(\"\\nüéØ HASIL PREDIKSI SENTIMEN DAN FUNGSI:\")\n",
        "    results = []\n",
        "    for comment, sentiment, function in zip(comments, sentiment_labels, function_labels):\n",
        "        print(f\"üó® Komentar: {comment}\\nüîπ Sentimen: {sentiment}\\nüîπ Fungsi: {function}\\n\")\n",
        "        results.append({\n",
        "            'comment': comment,\n",
        "            'predicted_sentiment': sentiment,\n",
        "            'predicted_function': function\n",
        "        })\n",
        "    return results"
      ],
      "metadata": {
        "id": "bwpBaPjgX3nl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rylqAVyyZEns"
      },
      "source": [
        "**4. PREDIKSI SENTIMEN GADGET**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    api_key = \"AIzaSyC216MP_3O1-VblW-zDAxweSUuAoRJ1U2I\"\n",
        "    video_id = \"Mari1pJzhWM\"\n",
        "    interval = 150  # Waktu tunggu (dalam detik)\n",
        "\n",
        "    print(\"üöÄ Memuat model dan vectorizer...\")\n",
        "    sentiment_model, function_model, vectorizer, tf_transformer, sentiment_encoder, function_encoder = load_models()\n",
        "\n",
        "    while True:\n",
        "        print(f\"\\n‚è≥ Mengambil komentar pada {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}...\")\n",
        "        comments = get_video_comments(api_key, video_id)\n",
        "\n",
        "        if not comments:\n",
        "            print(\"‚ö† Tidak ada komentar yang diambil. Menunggu periode berikutnya...\")\n",
        "        else:\n",
        "            print(\"\\nüìä Melakukan prediksi sentimen dan fungsi...\")\n",
        "            predictions = predict_comments(comments, sentiment_model, function_model, vectorizer, tf_transformer, sentiment_encoder, function_encoder)\n",
        "            predictions_df = pd.DataFrame(predictions)\n",
        "            print(\"\\n‚úÖ Prediksi selesai!\")\n",
        "\n",
        "            if not predictions_df.empty:\n",
        "                positif_df = predictions_df[predictions_df['predicted_sentiment'] == 'Positif']\n",
        "                counts = positif_df['predicted_function'].value_counts()\n",
        "\n",
        "                print(\"\\nüìà Jumlah komentar dengan sentimen positif per fungsi gadget:\")\n",
        "                for fungsi, jumlah in counts.items():\n",
        "                    print(f\"   - {fungsi}: {jumlah} komentar positif\")\n",
        "\n",
        "        print(f\"üïí Menunggu {interval / 60} menit sebelum mengambil komentar lagi...\\n\")\n",
        "        time.sleep(interval)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "zuHaU2XjNyPJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}