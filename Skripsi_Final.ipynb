{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "18PYryQEp19LRZ86iloTsAOLSuxWMUff0",
      "authorship_tag": "ABX9TyNKwEq8HekjSLUZbP1Xb8Ag",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dhani43/KNN-Model-TFRF-Dinamic-Crawling-Youtube/blob/main/Skripsi_Final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMFFTlnzPrJZ"
      },
      "source": [
        "**1. INSTALL REQUIREMENT**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yPFPDXrPzN5v"
      },
      "outputs": [],
      "source": [
        "!pip install pandas numpy scikit-learn openpyxl nltk google-api-python-client Sastrawi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IwL36FB9QaLC"
      },
      "source": [
        "**2. IMPORT REQUIREMENT**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "import pickle\n",
        "import os\n",
        "import time\n",
        "from datetime import datetime\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from google.colab import files, drive\n",
        "from googleapiclient.discovery import build"
      ],
      "metadata": {
        "id": "7WJsKzRVqNKD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. CRAWLING DATA**"
      ],
      "metadata": {
        "id": "UgrqrN61Pz6S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_video_comments(api_key, video_ids):\n",
        "    youtube = build('youtube', 'v3', developerKey=api_key)\n",
        "\n",
        "    # Inisialisasi list untuk menyimpan semua komentar dari semua video\n",
        "    all_comments = []\n",
        "\n",
        "    for video_id in video_ids:\n",
        "        # Inisialisasi parameter untuk paginasi\n",
        "        next_page_token = None\n",
        "\n",
        "        # Loop untuk mengambil semua halaman komentar dari video saat ini\n",
        "        while True:\n",
        "            try:\n",
        "                # Lakukan request untuk mendapatkan data komentar\n",
        "                response = youtube.commentThreads().list(\n",
        "                    part=\"snippet\",\n",
        "                    videoId=video_id,\n",
        "                    textFormat=\"plainText\",\n",
        "                    pageToken=next_page_token\n",
        "                ).execute()\n",
        "\n",
        "                # Lakukan loop untuk menambahkan komentar ke dalam list\n",
        "                for item in response[\"items\"]:\n",
        "                    comment = item[\"snippet\"][\"topLevelComment\"]\n",
        "                    author = comment[\"snippet\"][\"authorDisplayName\"]\n",
        "                    text = comment[\"snippet\"][\"textDisplay\"]\n",
        "                    published_at = comment[\"snippet\"][\"publishedAt\"]\n",
        "                    comment_time = datetime.strptime(published_at, \"%Y-%m-%dT%H:%M:%SZ\")\n",
        "                    all_comments.append({\"Time\": comment_time, \"Author\": author, \"Comment\": text})\n",
        "\n",
        "                    # Periksa apakah ada balasan untuk komentar ini\n",
        "                    if \"replies\" in item:\n",
        "                        for reply in item[\"replies\"][\"comments\"]:\n",
        "                            reply_author = reply[\"snippet\"][\"authorDisplayName\"]\n",
        "                            reply_text = reply[\"snippet\"][\"textDisplay\"]\n",
        "                            reply_published_at = reply[\"snippet\"][\"publishedAt\"]\n",
        "                            reply_time = datetime.strptime(reply_published_at, \"%Y-%m-%dT%H:%M:%SZ\")\n",
        "                            all_comments.append({\"Time\": reply_time, \"Author\": reply_author, \"Comment\": reply_text})\n",
        "\n",
        "                # Periksa apakah masih ada halaman komentar berikutnya\n",
        "                next_page_token = response.get(\"nextPageToken\")\n",
        "                if not next_page_token:\n",
        "                    break  # Keluar dari loop jika tidak ada halaman berikutnya\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"An error occurred: {e}\")\n",
        "                print(\"Retrying in 5 seconds...\")\n",
        "                time.sleep(30)  # Tunggu 30 detik sebelum mencoba lagi\n",
        "\n",
        "    # Simpan data komentar ke dalam file Excel\n",
        "    df = pd.DataFrame(all_comments)\n",
        "    df.to_excel(\"NewComments.xlsx\", index=False)\n",
        "    print(\"Comments saved to 'NewComments.xlsx'\")\n",
        "\n",
        "# Main function\n",
        "def main():\n",
        "    api_key = \"AIzaSyC216MP_3O1-VblW-zDAxweSUuAoRJ1U2I\"\n",
        "    video_ids = [\"c545HEI7OAU\", \"x7wSRgwMIpU\", \"l_kOERYYUkg\"]\n",
        "    get_video_comments(api_key, video_ids)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "IhhqUk0iUC3G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vBH2BiIQ9L5"
      },
      "source": [
        "**4. MENGHUBUNGKAN DENGAN GOOGLE DRIVE**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3hYOxW_FRMpK"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# Path penyimpanan model di Google Drive\n",
        "drive_path = \"/content/drive/My Drive/Model_Sentimen/\"\n",
        "os.makedirs(drive_path, exist_ok=True)  # Buat folder jika belum ada"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFEv3b5dRozn"
      },
      "source": [
        "**5. UNDUH RESOURCE NLTK**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WcTBDv_uR_KS"
      },
      "outputs": [],
      "source": [
        "# Unduh resource NLTK jika belum ada\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5cM9Hk0SHr2"
      },
      "source": [
        "**6. UPLOAD DAN MENAMPILKAN DATASET**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q69DavDUSVpd"
      },
      "outputs": [],
      "source": [
        "# Upload file\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Ambil nama file yang diunggah\n",
        "dataset_path = list(uploaded.keys())[0]\n",
        "df = pd.read_excel(dataset_path)\n",
        "\n",
        "display(df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBImQlWBSmVb"
      },
      "source": [
        "**7. PREPROCESSING DATA**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_column = 'Comment'\n",
        "sentiment_column = 'Sentimen'\n",
        "function_column = 'Fungsi'\n",
        "\n",
        "# Setup stemmer Sastrawi dan stopwords\n",
        "factory = StemmerFactory()\n",
        "stemmer = factory.create_stemmer()\n",
        "stop_words = set(stopwords.words('indonesian'))\n",
        "\n",
        "# Preprocessing\n",
        "def preprocesses_text(text):\n",
        "    # 1. Case Folding\n",
        "    casefolded_text = text.lower()\n",
        "    # 2. Cleaning\n",
        "    cleaned_text = re.sub(r'@\\w+|http\\S+|www\\.\\S+|<.*?>|[^\\w\\s]', ' ', casefolded_text)\n",
        "    cleaned_text = cleaned_text.strip()\n",
        "    # 3. Tokenizing\n",
        "    tokens = word_tokenize(cleaned_text)\n",
        "    # 4. Stopword removal\n",
        "    filtered = [word for word in tokens if word not in stop_words]\n",
        "    # 5. Stemming\n",
        "    stemmed = [stemmer.stem(word) for word in filtered]\n",
        "    # 6. Final text\n",
        "    final_text = ' '.join(stemmed)\n",
        "\n",
        "    return {\n",
        "        'casefolded_text': casefolded_text,\n",
        "        'cleaned_text': cleaned_text,\n",
        "        'tokens': tokens,\n",
        "        'filtered': filtered,\n",
        "        'stemmed': stemmed,\n",
        "        'final_text': final_text\n",
        "    }\n",
        "\n",
        "# Terapkan pre-processing\n",
        "df[text_column] = df[text_column].astype(str).apply(preprocesses_text)\n",
        "# Case Folding\n",
        "df['casefolded_text'] = df[text_column].apply(lambda x: x['casefolded_text'])\n",
        "# Cleaning\n",
        "df['cleaned_text'] = df[text_column].apply(lambda x: x['cleaned_text'])\n",
        "# Tokenizing\n",
        "df['tokens'] = df[text_column].apply(lambda x: x['tokens'])\n",
        "# Stopword Removal\n",
        "df['filtered'] = df[text_column].apply(lambda x: x['filtered'])\n",
        "# Stemming\n",
        "df['stemmed'] = df[text_column].apply(lambda x: x['stemmed'])\n",
        "df['final_text'] = df[text_column].apply(lambda x: x['final_text'])\n",
        "# Encoding labels\n",
        "sentiment_encoder = LabelEncoder()\n",
        "function_encoder = LabelEncoder()\n",
        "df['sentiment_label'] = sentiment_encoder.fit_transform(df[sentiment_column])\n",
        "df['function_label'] = function_encoder.fit_transform(df[function_column])\n",
        "\n",
        "# Tampilkan hasil pre-processing secara bertahap\n",
        "print(\"\\n✅ 1. HASIL CASE FOLDING :\")\n",
        "display(df[['casefolded_text']].head())\n",
        "\n",
        "print(\"\\n✅ 2. HASIL CLEANING :\")\n",
        "display(df[['cleaned_text']].head())\n",
        "\n",
        "print(\"\\n✅ 3. HASIL TOKENIZING :\")\n",
        "display(df[['tokens']].head())\n",
        "\n",
        "print(\"\\n✅ 4. HASIL STOPWORD REMOVAL :\")\n",
        "display(df[['filtered']].head())\n",
        "\n",
        "print(\"\\n✅ 5. HASIL STEMMING :\")\n",
        "display(df[['stemmed']].head())\n",
        "\n",
        "print(\"\\n✅ 6. TEKS FINAL :\")\n",
        "display(df[['final_text']].head())\n",
        "\n",
        "print(\"\\n✅ 7. ENCODING LABEL SENTIMEN :\")\n",
        "display(df[[sentiment_column, 'sentiment_label']].head())\n",
        "\n",
        "print(\"\\n✅ 8. ENCODING LABEL FUNGSI :\")\n",
        "display(df[[function_column, 'function_label']].head())"
      ],
      "metadata": {
        "id": "NrpLBuIocSar"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIQCMUVqVn6z"
      },
      "source": [
        "**8. PEMBOBOTAN KATA (TF-RF)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Q43Z940VxqO"
      },
      "outputs": [],
      "source": [
        "# TF-RF Vectorization\n",
        "vectorizer = CountVectorizer(ngram_range=(1,2))\n",
        "X_counts = vectorizer.fit_transform(df['final_text'])\n",
        "tf_transformer = TfidfTransformer(use_idf=False).fit(X_counts)\n",
        "X_tf = tf_transformer.transform(X_counts)\n",
        "df_counts = np.sum(X_counts.toarray() > 0, axis=0)\n",
        "n_docs = X_counts.shape[0]\n",
        "b = df_counts\n",
        "c = n_docs - b\n",
        "rf = np.log(2 + b / np.maximum(1, c))\n",
        "rf = rf.reshape(1, -1)\n",
        "X_tfrf = X_tf.multiply(rf)\n",
        "\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.expand_frame_repr', True)\n",
        "# Mendapatkan daftar fitur (kata/phrase) dari vectorizer\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "# Mengonversi hasil TF-RF (sparse matrix) ke bentuk array\n",
        "tfrf_array = X_tfrf.toarray()\n",
        "df_tfrf = pd.DataFrame(tfrf_array, columns=feature_names)\n",
        "print(df_tfrf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ivsr9_NQV2HE"
      },
      "source": [
        "**9. SPLIT DATA, MELATIH MODEL, MENAMPILKAN HASIL EVALUASI MODEL, MENYIMPAN MODEL**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cwzcrq03WYoH"
      },
      "outputs": [],
      "source": [
        "# Split data\n",
        "X_train, X_test, y_train_sentiment, y_test_sentiment = train_test_split(\n",
        "    X_tfrf, df['sentiment_label'], test_size=0.3, random_state=42)\n",
        "\n",
        "X_train_func, X_test_func, y_train_function, y_test_function = train_test_split(\n",
        "    X_tfrf, df['function_label'], test_size=0.3, random_state=42)\n",
        "\n",
        "# Train KNN models\n",
        "knn_sentiment = KNeighborsClassifier(n_neighbors=3, metric='cosine')\n",
        "knn_sentiment.fit(X_train, y_train_sentiment)\n",
        "\n",
        "knn_function = KNeighborsClassifier(n_neighbors=3, metric='cosine')\n",
        "knn_function.fit(X_train_func, y_train_function)\n",
        "\n",
        "# Evaluation\n",
        "y_pred_sentiment = knn_sentiment.predict(X_test)\n",
        "y_pred_function = knn_function.predict(X_test_func)\n",
        "\n",
        "print(\"\\n🎯 HASIL EVALUASI MODEL SENTIMEN:\")\n",
        "# Get unique labels in y_test_sentiment and y_pred_sentiment\n",
        "unique_labels = np.unique(np.concatenate((y_test_sentiment, y_pred_sentiment)))\n",
        "\n",
        "# Filter target names to include only the present labels\n",
        "target_names = [sentiment_encoder.classes_[i] for i in unique_labels]\n",
        "\n",
        "# Print the classification report with the filtered target names\n",
        "print(classification_report(y_test_sentiment, y_pred_sentiment, target_names=target_names))\n",
        "\n",
        "print(\"\\n🎯 HASIL EVALUASI MODEL FUNGSI:\")\n",
        "print(classification_report(y_test_function, y_pred_function, target_names=function_encoder.classes_))\n",
        "\n",
        "# Save models\n",
        "pickle.dump(knn_sentiment, open(drive_path + \"knn_sentiment.pkl\", \"wb\"))\n",
        "pickle.dump(knn_function, open(drive_path + \"knn_function.pkl\", \"wb\"))\n",
        "pickle.dump(vectorizer, open(drive_path + \"vectorizer.pkl\", \"wb\"))\n",
        "pickle.dump(tf_transformer, open(drive_path + \"tf_transformer.pkl\", \"wb\"))\n",
        "pickle.dump(sentiment_encoder, open(drive_path + \"sentiment_encoder.pkl\", \"wb\"))\n",
        "pickle.dump(function_encoder, open(drive_path + \"function_encoder.pkl\", \"wb\"))\n",
        "\n",
        "print(f\"\\n✅ Model berhasil disimpan di Google Drive: {drive_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0aIxgpQWxrG"
      },
      "source": [
        "**10. FUNGSI LOAD MODEL KNN, PREPROCESSING TEXT, CRAWLING DATA YOUTUBE, DAN PREDIKSI SENTIMEN**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fungsi untuk memuat model dan vectorizer\n",
        "def load_models():\n",
        "    with open(drive_path + \"knn_sentiment.pkl\", \"rb\") as f:\n",
        "        sentiment_model = pickle.load(f)\n",
        "    with open(drive_path + \"knn_function.pkl\", \"rb\") as f:\n",
        "        function_model = pickle.load(f)\n",
        "    with open(drive_path + \"vectorizer.pkl\", \"rb\") as f:\n",
        "        vectorizer = pickle.load(f)\n",
        "    with open(drive_path + \"tf_transformer.pkl\", \"rb\") as f:\n",
        "        tf_transformer = pickle.load(f)\n",
        "    with open(drive_path + \"sentiment_encoder.pkl\", \"rb\") as f:\n",
        "        sentiment_encoder = pickle.load(f)\n",
        "    with open(drive_path + \"function_encoder.pkl\", \"rb\") as f:\n",
        "        function_encoder = pickle.load(f)\n",
        "\n",
        "    print(\"✅ Model dan vectorizer berhasil dimuat!\")\n",
        "    return sentiment_model, function_model, vectorizer, tf_transformer, sentiment_encoder, function_encoder\n",
        "\n",
        "# Fungsi untuk membersihkan teks\n",
        "def preprocesses_text(text):\n",
        "    casefolded_text = text.lower()\n",
        "    cleaned_text = re.sub(r'@\\w+|http\\S+|www\\.\\S+|<.*?>|[^\\w\\s]', ' ', casefolded_text)\n",
        "    cleaned_text = cleaned_text.strip()\n",
        "    tokens = word_tokenize(cleaned_text)\n",
        "    filtered = [word for word in tokens if word not in stop_words]\n",
        "    stemmed = [stemmer.stem(word) for word in filtered]\n",
        "    final_text = ' '.join(stemmed)\n",
        "    return {\n",
        "        'casefolded_text': casefolded_text,\n",
        "        'cleaned_text': cleaned_text,\n",
        "        'tokens': tokens,\n",
        "        'filtered': filtered,\n",
        "        'stemmed': stemmed,\n",
        "        'final_text': final_text\n",
        "    }\n",
        "\n",
        "results = df[text_column].astype(str).apply(preprocesses_text)\n",
        "df['casefolded_text'] = results.apply(lambda x: x['casefolded_text'])\n",
        "df['cleaned_text'] = results.apply(lambda x: x['cleaned_text'])\n",
        "df['tokens'] = results.apply(lambda x: x['tokens'])\n",
        "df['filtered'] = results.apply(lambda x: x['filtered'])\n",
        "df['stemmed'] = results.apply(lambda x: x['stemmed'])\n",
        "df['final_text'] = results.apply(lambda x: x['final_text'])\n",
        "\n",
        "# Fungsi untuk mengambil komentar dari YouTube\n",
        "def get_video_comments(api_key, video_id):\n",
        "    try:\n",
        "        youtube = build('youtube', 'v3', developerKey=api_key)\n",
        "        all_comments = []\n",
        "        next_page_token = None\n",
        "        count = 0\n",
        "\n",
        "        while True:\n",
        "            response = youtube.commentThreads().list(\n",
        "                part=\"snippet\",\n",
        "                videoId=video_id,\n",
        "                textFormat=\"plainText\",\n",
        "                pageToken=next_page_token\n",
        "            ).execute()\n",
        "\n",
        "            for item in response.get(\"items\", []):\n",
        "                comment = item[\"snippet\"][\"topLevelComment\"][\"snippet\"][\"textDisplay\"]\n",
        "                processed = preprocesses_text(comment)\n",
        "                all_comments.append(processed['final_text'])\n",
        "                count += 1\n",
        "\n",
        "            next_page_token = response.get(\"nextPageToken\")\n",
        "            if not next_page_token:\n",
        "                break\n",
        "\n",
        "        print(f\"\\n✅ Berhasil mengambil {count} komentar dari video YouTube.\")\n",
        "\n",
        "        if not all_comments:\n",
        "            print(\"⚠ Tidak ada komentar yang ditemukan.\")\n",
        "\n",
        "        return all_comments\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Terjadi kesalahan saat mengambil komentar: {e}\")\n",
        "        return []\n",
        "\n",
        "# Fungsi untuk melakukan prediksi sentimen dan fungsi\n",
        "def predict_comments(comments, sentiment_model, function_model, vectorizer, tf_transformer, sentiment_encoder, function_encoder):\n",
        "    if not comments:\n",
        "        print(\"⚠ Tidak ada komentar yang dapat diprediksi.\")\n",
        "        return []\n",
        "\n",
        "    # Vectorisasi komentar\n",
        "    X_counts = vectorizer.transform(comments)\n",
        "    X_tf = tf_transformer.transform(X_counts)\n",
        "\n",
        "    # Prediksi sentimen\n",
        "    sentiment_predictions = sentiment_model.predict(X_tf)\n",
        "    sentiment_labels = sentiment_encoder.inverse_transform(sentiment_predictions)\n",
        "\n",
        "    # Prediksi fungsi\n",
        "    function_predictions = function_model.predict(X_tf)\n",
        "    function_labels = function_encoder.inverse_transform(function_predictions)\n",
        "\n",
        "    print(\"\\n🎯 HASIL PREDIKSI SENTIMEN DAN FUNGSI:\")\n",
        "    results = []\n",
        "    for comment, sentiment, function in zip(comments, sentiment_labels, function_labels):\n",
        "        print(f\"🗨 Komentar: {comment}\\n🔹 Sentimen: {sentiment}\\n🔹 Fungsi: {function}\\n\")\n",
        "        results.append({\n",
        "            'comment': comment,\n",
        "            'predicted_sentiment': sentiment,\n",
        "            'predicted_function': function\n",
        "        })\n",
        "    return results"
      ],
      "metadata": {
        "id": "bwpBaPjgX3nl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rylqAVyyZEns"
      },
      "source": [
        "**11. PREDIKSI SENTIMEN GADGET**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    api_key = \"AIzaSyC216MP_3O1-VblW-zDAxweSUuAoRJ1U2I\"\n",
        "    video_id = \"Mari1pJzhWM\"\n",
        "    interval = 150  # Waktu tunggu (dalam detik)\n",
        "\n",
        "    print(\"🚀 Memuat model dan vectorizer...\")\n",
        "    sentiment_model, function_model, vectorizer, tf_transformer, sentiment_encoder, function_encoder = load_models()\n",
        "\n",
        "    while True:\n",
        "        print(f\"\\n⏳ Mengambil komentar pada {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}...\")\n",
        "        comments = get_video_comments(api_key, video_id)\n",
        "\n",
        "        if not comments:\n",
        "            print(\"⚠ Tidak ada komentar yang diambil. Menunggu periode berikutnya...\")\n",
        "        else:\n",
        "            print(\"\\n📊 Melakukan prediksi sentimen dan fungsi...\")\n",
        "            predictions = predict_comments(comments, sentiment_model, function_model, vectorizer, tf_transformer, sentiment_encoder, function_encoder)\n",
        "            predictions_df = pd.DataFrame(predictions)\n",
        "            print(\"\\n✅ Prediksi selesai!\")\n",
        "\n",
        "            if not predictions_df.empty:\n",
        "                positif_df = predictions_df[predictions_df['predicted_sentiment'] == 'Positif']\n",
        "                counts = positif_df['predicted_function'].value_counts()\n",
        "\n",
        "                print(\"\\n📈 Jumlah komentar dengan sentimen positif per fungsi gadget:\")\n",
        "                for fungsi, jumlah in counts.items():\n",
        "                    print(f\"   - {fungsi}: {jumlah} komentar positif\")\n",
        "\n",
        "        print(f\"🕒 Menunggu {interval / 60} menit sebelum mengambil komentar lagi...\\n\")\n",
        "        time.sleep(interval)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "zuHaU2XjNyPJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}