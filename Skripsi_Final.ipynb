{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNkEpWCMY4UDphK8Lci2BOW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dhani43/KNN-Model-TFRF-Dinamic-Crawling-Youtube/blob/main/Skripsi_Final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMFFTlnzPrJZ"
      },
      "source": [
        "**1. INSTALL REQUIREMENT**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yPFPDXrPzN5v",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install pandas numpy scikit-learn openpyxl nltk google-api-python-client Sastrawi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IwL36FB9QaLC"
      },
      "source": [
        "**2. IMPORT REQUIREMENT**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "import pickle\n",
        "import os\n",
        "import time\n",
        "from datetime import datetime\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from google.colab import files, drive\n",
        "from googleapiclient.discovery import build"
      ],
      "metadata": {
        "id": "7WJsKzRVqNKD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vBH2BiIQ9L5"
      },
      "source": [
        "**3. MENGHUBUNGKAN DENGAN GOOGLE DRIVE**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3hYOxW_FRMpK",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Path penyimpanan model di Google Drive\n",
        "drive_path = \"/content/drive/My Drive/Model_Sentimen/\"\n",
        "os.makedirs(drive_path, exist_ok=True)  # Buat folder jika belum ada"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFEv3b5dRozn"
      },
      "source": [
        "**4. UNDUH RESOURCE NLTK**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WcTBDv_uR_KS",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Unduh resource NLTK jika belum ada\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5cM9Hk0SHr2"
      },
      "source": [
        "**5. UPLOAD DAN MENAMPILKAN DATASET**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q69DavDUSVpd",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Upload file\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Ambil nama file yang diunggah\n",
        "dataset_path = list(uploaded.keys())[0]\n",
        "df = pd.read_excel(dataset_path)\n",
        "\n",
        "display(df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBImQlWBSmVb"
      },
      "source": [
        "**6. PREPROCESSING DATA**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_column = 'Comment'\n",
        "sentiment_column = 'Sentimen'\n",
        "function_column = 'Fungsi'\n",
        "\n",
        "# Setup stemmer Sastrawi\n",
        "factory = StemmerFactory()\n",
        "stemmer = factory.create_stemmer()\n",
        "\n",
        "# Preprocessing\n",
        "def preprocesses_text(text):\n",
        "    # Case Folding\n",
        "    text = text.lower()\n",
        "\n",
        "    # Cleaning\n",
        "    text = re.sub(r'@\\w+|http\\S+|www\\.\\S+|<.*?>|[^\\w\\s]', ' ', text)\n",
        "    text = text.strip()\n",
        "    return text\n",
        "\n",
        "# Terapkan pre-processing\n",
        "df[text_column] = df[text_column].astype(str).apply(preprocesses_text)\n",
        "\n",
        "# Tokenizing\n",
        "df['tokens'] = df[text_column].apply(word_tokenize)\n",
        "\n",
        "# Stopword Removal\n",
        "stop_words = set(stopwords.words('indonesian'))\n",
        "df['filtered'] = df['tokens'].apply(lambda x: [word for word in x if word not in stop_words])\n",
        "\n",
        "# Stemming\n",
        "df['stemmed'] = df['filtered'].apply(lambda x: [stemmer.stem(word) for word in x])\n",
        "df['final_text'] = df['stemmed'].apply(lambda x: ' '.join(x))\n",
        "\n",
        "# Encoding labels\n",
        "sentiment_encoder = LabelEncoder()\n",
        "function_encoder = LabelEncoder()\n",
        "df['sentiment_label'] = sentiment_encoder.fit_transform(df[sentiment_column])\n",
        "df['function_label'] = function_encoder.fit_transform(df[function_column])\n",
        "\n",
        "# Tampilkan hasil pre-processing secara bertahap\n",
        "print(\"\\n‚úÖ 1. HASIL CASE FOLDING & CLEANING :\")\n",
        "display(df[[text_column]].head())\n",
        "\n",
        "print(\"\\n‚úÖ 2. HASIL TOKENIZING :\")\n",
        "display(df[['tokens']].head())\n",
        "\n",
        "print(\"\\n‚úÖ 3. HASIL STOPWORD REMOVAL :\")\n",
        "display(df[['filtered']].head())\n",
        "\n",
        "print(\"\\n‚úÖ 4. HASIL STEMMING :\")\n",
        "display(df[['stemmed']].head())\n",
        "\n",
        "print(\"\\n‚úÖ 5. TEKS FINAL :\")\n",
        "display(df[['final_text']].head())\n",
        "\n",
        "print(\"\\n‚úÖ 6. ENCODING LABEL SENTIMEN :\")\n",
        "display(df[[sentiment_column, 'sentiment_label']].head())\n",
        "\n",
        "print(\"\\n‚úÖ 7. ENCODING LABEL FUNGSI :\")\n",
        "display(df[[function_column, 'function_label']].head())"
      ],
      "metadata": {
        "id": "uuveLTSHDgL_",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIQCMUVqVn6z"
      },
      "source": [
        "**7. PEMBOBOTAN KATA (TF-RF)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Q43Z940VxqO"
      },
      "outputs": [],
      "source": [
        "# TF-RF Vectorization\n",
        "vectorizer = CountVectorizer(ngram_range=(1,2))\n",
        "X_counts = vectorizer.fit_transform(df['final_text'])\n",
        "tf_transformer = TfidfTransformer(use_idf=False).fit(X_counts)\n",
        "X_tf = tf_transformer.transform(X_counts)\n",
        "df_counts = np.sum(X_counts.toarray() > 0, axis=0)\n",
        "n_docs = X_counts.shape[0]\n",
        "b = df_counts\n",
        "c = n_docs - b\n",
        "rf = np.log(2 + b / np.maximum(1, c))\n",
        "rf = rf.reshape(1, -1)\n",
        "X_tfrf = X_tf.multiply(rf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ivsr9_NQV2HE"
      },
      "source": [
        "**8. SPLIT DATA, MELATIH MODEL, MENAMPILKAN HASIL EVALUASI MODEL, MENYIMPAN MODEL**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cwzcrq03WYoH",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Split data\n",
        "X_train, X_test, y_train_sentiment, y_test_sentiment = train_test_split(\n",
        "    X_tfrf, df['sentiment_label'], test_size=0.3, random_state=42)\n",
        "\n",
        "X_train_func, X_test_func, y_train_function, y_test_function = train_test_split(\n",
        "    X_tfrf, df['function_label'], test_size=0.3, random_state=42)\n",
        "\n",
        "# Train KNN models\n",
        "knn_sentiment = KNeighborsClassifier(n_neighbors=3, metric='cosine')\n",
        "knn_sentiment.fit(X_train, y_train_sentiment)\n",
        "\n",
        "knn_function = KNeighborsClassifier(n_neighbors=3, metric='cosine')\n",
        "knn_function.fit(X_train_func, y_train_function)\n",
        "\n",
        "# Evaluation\n",
        "y_pred_sentiment = knn_sentiment.predict(X_test)\n",
        "y_pred_function = knn_function.predict(X_test_func)\n",
        "\n",
        "print(\"\\nüéØ HASIL EVALUASI MODEL SENTIMEN:\")\n",
        "# Get unique labels in y_test_sentiment and y_pred_sentiment\n",
        "unique_labels = np.unique(np.concatenate((y_test_sentiment, y_pred_sentiment)))\n",
        "\n",
        "# Filter target names to include only the present labels\n",
        "target_names = [sentiment_encoder.classes_[i] for i in unique_labels]\n",
        "\n",
        "# Print the classification report with the filtered target names\n",
        "print(classification_report(y_test_sentiment, y_pred_sentiment, target_names=target_names))\n",
        "\n",
        "print(\"\\nüéØ HASIL EVALUASI MODEL FUNGSI:\")\n",
        "print(classification_report(y_test_function, y_pred_function, target_names=function_encoder.classes_))\n",
        "\n",
        "# Save models\n",
        "pickle.dump(knn_sentiment, open(drive_path + \"knn_sentiment.pkl\", \"wb\"))\n",
        "pickle.dump(knn_function, open(drive_path + \"knn_function.pkl\", \"wb\"))\n",
        "pickle.dump(vectorizer, open(drive_path + \"vectorizer.pkl\", \"wb\"))\n",
        "pickle.dump(tf_transformer, open(drive_path + \"tf_transformer.pkl\", \"wb\"))\n",
        "pickle.dump(sentiment_encoder, open(drive_path + \"sentiment_encoder.pkl\", \"wb\"))\n",
        "pickle.dump(function_encoder, open(drive_path + \"function_encoder.pkl\", \"wb\"))\n",
        "\n",
        "print(f\"\\n‚úÖ Model berhasil disimpan di Google Drive: {drive_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0aIxgpQWxrG"
      },
      "source": [
        "**9. FUNGSI LOAD MODEL KNN, PREPROCESSING TEXT, CRAWLING DATA YOUTUBE, DAN PREDIKSI SENTIMEN**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CKg4ESwyXr6h"
      },
      "outputs": [],
      "source": [
        "# Fungsi untuk memuat model dan vectorizer\n",
        "def load_models():\n",
        "    with open(drive_path + \"knn_sentiment.pkl\", \"rb\") as f:\n",
        "        sentiment_model = pickle.load(f)\n",
        "    with open(drive_path + \"knn_function.pkl\", \"rb\") as f:\n",
        "        function_model = pickle.load(f)\n",
        "    with open(drive_path + \"vectorizer.pkl\", \"rb\") as f:\n",
        "        vectorizer = pickle.load(f)\n",
        "    with open(drive_path + \"tf_transformer.pkl\", \"rb\") as f:\n",
        "        tf_transformer = pickle.load(f)\n",
        "    with open(drive_path + \"sentiment_encoder.pkl\", \"rb\") as f:\n",
        "        sentiment_encoder = pickle.load(f)\n",
        "    with open(drive_path + \"function_encoder.pkl\", \"rb\") as f:\n",
        "        function_encoder = pickle.load(f)\n",
        "\n",
        "    print(\"‚úÖ Model dan vectorizer berhasil dimuat!\")\n",
        "    return sentiment_model, function_model, vectorizer, tf_transformer, sentiment_encoder, function_encoder\n",
        "\n",
        "# Fungsi untuk membersihkan teks\n",
        "def preprocesses_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'@\\w+|http\\S+|www\\.\\S+|<.*?>|[^\\w\\s]', ' ', text)\n",
        "    text = text.strip()\n",
        "    return text\n",
        "\n",
        "df[text_column] = df[text_column].astype(str).apply(preprocesses_text)\n",
        "df['tokens'] = df[text_column].apply(word_tokenize)\n",
        "stop_words = set(stopwords.words('indonesian'))\n",
        "df['filtered'] = df['tokens'].apply(lambda x: [word for word in x if word not in stop_words])\n",
        "df['stemmed'] = df['filtered'].apply(lambda x: [stemmer.stem(word) for word in x])\n",
        "df['final_text'] = df['stemmed'].apply(lambda x: ' '.join(x))\n",
        "\n",
        "# Fungsi untuk mengambil komentar dari YouTube\n",
        "def get_video_comments(api_key, video_id):\n",
        "    try:\n",
        "        youtube = build('youtube', 'v3', developerKey=api_key)\n",
        "        all_comments = []\n",
        "        next_page_token = None\n",
        "        count = 0\n",
        "\n",
        "        while True:\n",
        "            response = youtube.commentThreads().list(\n",
        "                part=\"snippet\",\n",
        "                videoId=video_id,\n",
        "                textFormat=\"plainText\",\n",
        "                pageToken=next_page_token\n",
        "            ).execute()\n",
        "\n",
        "            for item in response.get(\"items\", []):\n",
        "                comment = item[\"snippet\"][\"topLevelComment\"][\"snippet\"][\"textDisplay\"]\n",
        "                cleaned_comment = preprocesses_text(comment)\n",
        "                all_comments.append(cleaned_comment)\n",
        "                count += 1\n",
        "\n",
        "            next_page_token = response.get(\"nextPageToken\")\n",
        "            if not next_page_token:\n",
        "                break\n",
        "\n",
        "        print(f\"\\n‚úÖ Berhasil mengambil {count} komentar dari video YouTube.\")\n",
        "\n",
        "        if not all_comments:\n",
        "            print(\"‚ö† Tidak ada komentar yang ditemukan.\")\n",
        "\n",
        "        return all_comments\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Terjadi kesalahan saat mengambil komentar: {e}\")\n",
        "        return []\n",
        "\n",
        "# Fungsi untuk melakukan prediksi sentimen dan fungsi\n",
        "def predict_comments(comments, sentiment_model, function_model, vectorizer, tf_transformer, sentiment_encoder, function_encoder):\n",
        "    if not comments:\n",
        "        print(\"‚ö† Tidak ada komentar yang dapat diprediksi.\")\n",
        "        return []\n",
        "\n",
        "    print(\"\\nüîπ Melakukan preprocessing untuk komentar yang diambil...\")\n",
        "\n",
        "    # Vectorisasi komentar\n",
        "    X_counts = vectorizer.transform(comments)\n",
        "    X_tf = tf_transformer.transform(X_counts)\n",
        "\n",
        "    # Prediksi sentimen\n",
        "    sentiment_predictions = sentiment_model.predict(X_tf)\n",
        "    sentiment_labels = sentiment_encoder.inverse_transform(sentiment_predictions)\n",
        "\n",
        "    # Prediksi fungsi\n",
        "    function_predictions = function_model.predict(X_tf)\n",
        "    function_labels = function_encoder.inverse_transform(function_predictions)\n",
        "\n",
        "    print(\"\\nüéØ HASIL PREDIKSI SENTIMEN DAN FUNGSI:\")\n",
        "    for comment, sentiment, function in zip(comments, sentiment_labels, function_labels):\n",
        "        print(f\"üó® Komentar: {comment}\\nüîπ Sentimen: {sentiment}\\nüîπ Fungsi: {function}\\n\")\n",
        "\n",
        "    return list(zip(comments, sentiment_labels, function_labels))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rylqAVyyZEns"
      },
      "source": [
        "**10. PREDIKSI SENTIMEN GADGET**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X01BjnqNZDwa",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Fungsi utama untuk menjalankan program secara periodik\n",
        "def main():\n",
        "    api_key = \"Change With Your API KEY\"\n",
        "    video_id = \"Change With Your Video Id\"\n",
        "    interval = 150  # Waktu tunggu (dalam detik)\n",
        "\n",
        "    print(\"üöÄ Memuat model dan vectorizer...\")\n",
        "    sentiment_model, function_model, vectorizer, tf_transformer, sentiment_encoder, function_encoder = load_models()\n",
        "\n",
        "    while True:\n",
        "        print(f\"\\n‚è≥ Mengambil komentar pada {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}...\")\n",
        "        comments = get_video_comments(api_key, video_id)\n",
        "\n",
        "        if not comments:\n",
        "            print(\"‚ö† Tidak ada komentar yang diambil. Menunggu periode berikutnya...\")\n",
        "        else:\n",
        "            print(\"\\nüìä Melakukan prediksi sentimen dan fungsi...\")\n",
        "            predictions = predict_comments(comments, sentiment_model, function_model, vectorizer, tf_transformer, sentiment_encoder, function_encoder)\n",
        "            print(\"\\n‚úÖ Prediksi selesai!\")\n",
        "\n",
        "        print(f\"üïí Menunggu {interval / 60} menit sebelum mengambil komentar lagi...\\n\")\n",
        "        time.sleep(interval)  # Tunggu sebelum mengambil komentar lagi\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}
