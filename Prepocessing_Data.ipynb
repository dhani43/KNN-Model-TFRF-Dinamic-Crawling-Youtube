{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMettJkK1sAscaOxdQiVAya",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dhani43/KNN-Model-TFRF-Dinamic-Crawling-Youtube/blob/main/Prepocessing_Data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMFFTlnzPrJZ"
      },
      "source": [
        "**1. INSTALL REQUIREMENT**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yPFPDXrPzN5v"
      },
      "outputs": [],
      "source": [
        "!pip install pandas numpy scikit-learn openpyxl nltk google-api-python-client Sastrawi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IwL36FB9QaLC"
      },
      "source": [
        "**2. IMPORT REQUIREMENT**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "import pickle\n",
        "import os\n",
        "import time\n",
        "from datetime import datetime\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from google.colab import files, drive\n",
        "from googleapiclient.discovery import build"
      ],
      "metadata": {
        "id": "7WJsKzRVqNKD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vBH2BiIQ9L5"
      },
      "source": [
        "**3. MENGHUBUNGKAN DENGAN GOOGLE DRIVE**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3hYOxW_FRMpK"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# Path penyimpanan model di Google Drive\n",
        "drive_path = \"/content/drive/My Drive/Model_Sentimen/\"\n",
        "os.makedirs(drive_path, exist_ok=True)  # Buat folder jika belum ada"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFEv3b5dRozn"
      },
      "source": [
        "**4. UNDUH RESOURCE NLTK**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WcTBDv_uR_KS"
      },
      "outputs": [],
      "source": [
        "# Unduh resource NLTK jika belum ada\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5cM9Hk0SHr2"
      },
      "source": [
        "**5. UPLOAD DAN MENAMPILKAN DATASET**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q69DavDUSVpd"
      },
      "outputs": [],
      "source": [
        "# Upload file\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Ambil nama file yang diunggah\n",
        "dataset_path = list(uploaded.keys())[0]\n",
        "df = pd.read_excel(dataset_path)\n",
        "\n",
        "display(df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBImQlWBSmVb"
      },
      "source": [
        "**6. PREPROCESSING DATA**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_column = 'Comment'\n",
        "sentiment_column = 'Sentimen'\n",
        "function_column = 'Fungsi'\n",
        "\n",
        "# Setup stemmer Sastrawi dan stopwords\n",
        "factory = StemmerFactory()\n",
        "stemmer = factory.create_stemmer()\n",
        "stop_words = set(stopwords.words('indonesian'))\n",
        "\n",
        "# Preprocessing\n",
        "def preprocesses_text(text):\n",
        "    # 1. Case Folding\n",
        "    casefolded_text = text.lower()\n",
        "    # 2. Cleaning\n",
        "    cleaned_text = re.sub(r'@\\w+|http\\S+|www\\.\\S+|<.*?>|[^\\w\\s]', ' ', casefolded_text)\n",
        "    cleaned_text = cleaned_text.strip()\n",
        "    # 3. Tokenizing\n",
        "    tokens = word_tokenize(cleaned_text)\n",
        "    # 4. Stopword removal\n",
        "    filtered = [word for word in tokens if word not in stop_words]\n",
        "    # 5. Stemming\n",
        "    stemmed = [stemmer.stem(word) for word in filtered]\n",
        "    # 6. Final text\n",
        "    final_text = ' '.join(stemmed)\n",
        "\n",
        "    return {\n",
        "        'casefolded_text': casefolded_text,\n",
        "        'cleaned_text': cleaned_text,\n",
        "        'tokens': tokens,\n",
        "        'filtered': filtered,\n",
        "        'stemmed': stemmed,\n",
        "        'final_text': final_text\n",
        "    }\n",
        "\n",
        "# Terapkan pre-processing\n",
        "df[text_column] = df[text_column].astype(str).apply(preprocesses_text)\n",
        "# Case Folding\n",
        "df['casefolded_text'] = df[text_column].apply(lambda x: x['casefolded_text'])\n",
        "# Cleaning\n",
        "df['cleaned_text'] = df[text_column].apply(lambda x: x['cleaned_text'])\n",
        "# Tokenizing\n",
        "df['tokens'] = df[text_column].apply(lambda x: x['tokens'])\n",
        "# Stopword Removal\n",
        "df['filtered'] = df[text_column].apply(lambda x: x['filtered'])\n",
        "# Stemming\n",
        "df['stemmed'] = df[text_column].apply(lambda x: x['stemmed'])\n",
        "df['final_text'] = df[text_column].apply(lambda x: x['final_text'])\n",
        "# Encoding labels\n",
        "sentiment_encoder = LabelEncoder()\n",
        "function_encoder = LabelEncoder()\n",
        "df['sentiment_label'] = sentiment_encoder.fit_transform(df[sentiment_column])\n",
        "df['function_label'] = function_encoder.fit_transform(df[function_column])\n",
        "\n",
        "# Tampilkan hasil pre-processing secara bertahap\n",
        "print(\"\\n✅ 1. HASIL CASE FOLDING :\")\n",
        "display(df[['casefolded_text']].head())\n",
        "\n",
        "print(\"\\n✅ 2. HASIL CLEANING :\")\n",
        "display(df[['cleaned_text']].head())\n",
        "\n",
        "print(\"\\n✅ 3. HASIL TOKENIZING :\")\n",
        "display(df[['tokens']].head())\n",
        "\n",
        "print(\"\\n✅ 4. HASIL STOPWORD REMOVAL :\")\n",
        "display(df[['filtered']].head())\n",
        "\n",
        "print(\"\\n✅ 5. HASIL STEMMING :\")\n",
        "display(df[['stemmed']].head())\n",
        "\n",
        "print(\"\\n✅ 6. TEKS FINAL :\")\n",
        "display(df[['final_text']].head())\n",
        "\n",
        "print(\"\\n✅ 7. ENCODING LABEL SENTIMEN :\")\n",
        "display(df[[sentiment_column, 'sentiment_label']].head())\n",
        "\n",
        "print(\"\\n✅ 8. ENCODING LABEL FUNGSI :\")\n",
        "display(df[[function_column, 'function_label']].head())"
      ],
      "metadata": {
        "id": "NrpLBuIocSar"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}